#!/usr/bin/env python3
"""
NGãƒ¯ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (FAISSç‰ˆ)
input.csvã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ã—ã¦å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
"""

import pandas as pd
import os
import json
from datetime import datetime
import logging
import pickle
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# FAISSã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import faiss
except ImportError:
    print("FAISSãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("pip install faiss-cpu")
    exit(1)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SimpleJapaneseVectorizer:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªæ—¥æœ¬èªãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        # æ—¥æœ¬èªå¯¾å¿œã®TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼
        self.vectorizer = TfidfVectorizer(
            analyzer='char',  # æ–‡å­—ãƒ¬ãƒ™ãƒ«ã§ã®è§£æ
            ngram_range=(2, 4),  # 2-4æ–‡å­—ã®n-gram
            max_features=5000,  # ç‰¹å¾´é‡ã®æœ€å¤§æ•°
            lowercase=False,  # æ—¥æœ¬èªã§ã¯å°æ–‡å­—å¤‰æ›ã¯ä¸è¦
            token_pattern=None  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ç„¡åŠ¹åŒ–
        )
        self.fitted = False
    
    def fit_transform(self, texts):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆè¨“ç·´+å¤‰æ›ï¼‰"""
        vectors = self.vectorizer.fit_transform(texts)
        self.fitted = True
        return vectors.toarray()  # å¯†ãªé…åˆ—ã«å¤‰æ›
    
    def transform(self, texts):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆå¤‰æ›ã®ã¿ï¼‰"""
        if not self.fitted:
            raise ValueError("Vectorizer has not been fitted yet")
        vectors = self.vectorizer.transform(texts)
        return vectors.toarray()

def setup_ngword_faiss():
    """NGãƒ¯ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’FAISSã§æ§‹ç¯‰"""
    try:
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        logger.info("CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–‹å§‹...")
        if not os.path.exists('input.csv'):
            logger.error("input.csvãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
            
        df = pd.read_csv('input.csv', encoding='utf-8')
        logger.info(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶ã®NGãƒ¯ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿")
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
        logger.info("ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–...")
        vectorizer = SimpleJapaneseVectorizer()
        logger.info("ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–å®Œäº†")
        
        # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        ng_words = df['ng_word'].tolist()
        replacements = df['replacement'].tolist()
        reasons = df['reason'].tolist()
        risk_levels = df['risk_level'].tolist()
        
        # å„NGãƒ¯ãƒ¼ãƒ‰ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
        logger.info("TF-IDFãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆä¸­...")
        embeddings = vectorizer.fit_transform(ng_words)
        logger.info(f"TF-IDFãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆå®Œäº†: {embeddings.shape}")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        metadatas = []
        for i, (ng_word, replacement, reason, risk_level) in enumerate(zip(ng_words, replacements, reasons, risk_levels)):
            metadata = {
                "id": i,
                "ng_word": ng_word,
                "replacement": replacement, 
                "reason": reason,
                "risk_level": risk_level,
                "created_at": datetime.now().isoformat()
            }
            metadatas.append(metadata)
        
        # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
        logger.info("FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ä¸­...")
        dimension = embeddings.shape[1]
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ç”¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆL2æ­£è¦åŒ– + å†…ç©ï¼‰
        index = faiss.IndexFlatIP(dimension)
        
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚’L2æ­£è¦åŒ–ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®ãŸã‚ï¼‰
        embeddings_normalized = embeddings.astype('float32')
        faiss.normalize_L2(embeddings_normalized)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ 
        index.add(embeddings_normalized)
        logger.info(f"FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å®Œäº†: {index.ntotal}ä»¶ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ ")
        
        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        ngword_data = {
            'faiss_index': index,
            'vectorizer': vectorizer,
            'metadatas': metadatas,
            'config': {
                'total_records': len(ng_words),
                'vector_dim': dimension,
                'created_at': datetime.now().isoformat(),
                'description': 'NGãƒ¯ãƒ¼ãƒ‰FAISSãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆTF-IDF + ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰'
            }
        }
        
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        output_file = "ngword_faiss.pkl"
        if os.path.exists(output_file):
            os.remove(output_file)
            logger.info("æ—¢å­˜ã®FAISSãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        logger.info("FAISSãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ä¸­...")
        with open(output_file, 'wb') as f:
            pickle.dump(ngword_data, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
        file_size = os.path.getsize(output_file)
        logger.info(f"ä¿å­˜å®Œäº†: {output_file} ({file_size:,} bytes)")
        
        # è¨­å®šæƒ…å ±ã‚’JSONå½¢å¼ã§ã‚‚ä¿å­˜ï¼ˆç¢ºèªç”¨ï¼‰
        config_file = "ngword_faiss_config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(ngword_data['config'], f, ensure_ascii=False, indent=2)
        logger.info(f"è¨­å®šæƒ…å ±ä¿å­˜: {config_file}")
        
        # ãƒ†ã‚¹ãƒˆæ¤œç´¢ã®å®Ÿè¡Œ
        logger.info("ãƒ†ã‚¹ãƒˆæ¤œç´¢å®Ÿè¡Œä¸­...")
        test_query = "ç¾ç™½åŠ¹æœ"
        query_vector = vectorizer.transform([test_query]).astype('float32')
        faiss.normalize_L2(query_vector)
        
        distances, indices = index.search(query_vector, 5)
        logger.info(f"ãƒ†ã‚¹ãƒˆæ¤œç´¢çµæœ (ã‚¯ã‚¨ãƒª: '{test_query}'):")
        for i, (idx, distance) in enumerate(zip(indices[0], distances[0])):
            if idx >= 0:  # æœ‰åŠ¹ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                similarity = distance  # æ­£è¦åŒ–æ¸ˆã¿å†…ç© = ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
                ng_word = metadatas[idx]['ng_word']
                logger.info(f"  {i+1}. {ng_word} (é¡ä¼¼åº¦: {similarity:.3f})")
        
        logger.info("âœ… NGãƒ¯ãƒ¼ãƒ‰FAISSãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸš€ NGãƒ¯ãƒ¼ãƒ‰FAISSãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ã‚’é–‹å§‹ã—ã¾ã™...")
    success = setup_ngword_faiss()
    
    if success:
        print("\nâœ… æ§‹ç¯‰å®Œäº†ï¼ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ:")
        print("  - ngword_faiss.pkl (ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹)")
        print("  - ngword_faiss_config.json (è¨­å®šæƒ…å ±)")
        print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("  1. requirements.txtã‚’æ›´æ–°ã—ã¦faiss-cpuã‚’è¿½åŠ ")
        print("  2. app.pyã‚’FAISSå¯¾å¿œã«æ›´æ–°")
        print("  3. å¤ã„ChromaDBãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤")
    else:
        print("\nâŒ æ§‹ç¯‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        exit(1)