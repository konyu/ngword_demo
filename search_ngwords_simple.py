#!/usr/bin/env python3
"""
ã‚·ãƒ³ãƒ—ãƒ«NGãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ãƒ»å‹•ä½œç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆ
TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’ä½¿ã£ãŸChromaDBã§ã®NGãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
"""

import streamlit as st
import chromadb
import json
import os
import pandas as pd
import pickle
from datetime import datetime
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

class SimpleJapaneseVectorizer:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªæ—¥æœ¬èªãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        # æ—¥æœ¬èªå¯¾å¿œã®TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼
        self.vectorizer = TfidfVectorizer(
            analyzer='char',  # æ–‡å­—ãƒ¬ãƒ™ãƒ«ã§ã®è§£æ
            ngram_range=(2, 4),  # 2-4æ–‡å­—ã®n-gram
            max_features=5000,  # ç‰¹å¾´é‡ã®æœ€å¤§æ•°
            lowercase=False,  # æ—¥æœ¬èªã§ã¯å°æ–‡å­—å¤‰æ›ã¯ä¸è¦
            token_pattern=None  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ç„¡åŠ¹åŒ–
        )
        self.fitted = False

    def fit_transform(self, texts):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆè¨“ç·´+å¤‰æ›ï¼‰"""
        vectors = self.vectorizer.fit_transform(texts)
        self.fitted = True
        return vectors.toarray()  # å¯†ãªé…åˆ—ã«å¤‰æ›

    def transform(self, texts):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆå¤‰æ›ã®ã¿ï¼‰"""
        if not self.fitted:
            raise ValueError("Vectorizer has not been fitted yet")
        vectors = self.vectorizer.transform(texts)
        return vectors.toarray()

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="NGãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  (TF-IDFç‰ˆ)",
    page_icon="ğŸš«",
    layout="wide"
)

@st.cache_resource
def load_vectorizer_and_db():
    """ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãï¼‰"""
    try:
        # TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
        with open("tfidf_vectorizer.pkl", "rb") as f:
            vectorizer = pickle.load(f)

        # ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        chroma_client = chromadb.PersistentClient(path="./chroma_db")
        collection = chroma_client.get_collection("ng_words_simple")

        return vectorizer, collection
    except Exception as e:
        st.error(f"ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        return None, None

def search_ng_words(query, vectorizer, collection, threshold=0.3, max_results=10):
    """NGãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢"""
    try:
        # ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        query_embedding = vectorizer.transform([query])

        # ChromaDBã§é¡ä¼¼æ¤œç´¢
        results = collection.query(
            query_embeddings=query_embedding.tolist(),
            n_results=max_results
        )

        # çµæœã‚’æ•´ç†
        search_results = []
        for i, (doc, metadata, distance) in enumerate(zip(
            results['documents'][0],
            results['metadatas'][0],
            results['distances'][0]
        )):
            similarity = 1 - distance  # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã«å¤‰æ›

            if similarity >= threshold:
                result = {
                    'ng_word': metadata['ng_word'],
                    'replacement': metadata['replacement'],
                    'reason': metadata['reason'],
                    'risk_level': metadata['risk_level'],
                    'similarity': similarity,
                    'distance': distance
                }
                search_results.append(result)

        return search_results

    except Exception as e:
        st.error(f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return []

def display_search_results(results, query):
    """æ¤œç´¢çµæœã‚’è¡¨ç¤º"""
    if not results:
        st.warning("è©²å½“ã™ã‚‹NGãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        st.info("ğŸ’¡ é¡ä¼¼åº¦ã—ãã„å€¤ã‚’ä¸‹ã’ã‚‹ã‹ã€åˆ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")
        return

    st.success(f"ğŸ¯ '{query}' ã«é¡ä¼¼ã™ã‚‹NGãƒ¯ãƒ¼ãƒ‰ã‚’ {len(results)} ä»¶ç™ºè¦‹ã—ã¾ã—ãŸ")

    # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«åˆ¥ã«è‰²åˆ†ã‘
    risk_colors = {
        'high': 'ğŸ”´',
        'mid': 'ğŸŸ¡',
        'low': 'ğŸŸ¢'
    }

    for i, result in enumerate(results, 1):
        risk_icon = risk_colors.get(result['risk_level'], 'âšª')

        with st.expander(
            f"{risk_icon} {i}. ã€Œ{result['ng_word']}ã€ (é¡ä¼¼åº¦: {result['similarity']:.3f})",
            expanded=i<=3  # ä¸Šä½3ä»¶ã¯å±•é–‹è¡¨ç¤º
        ):
            col1, col2 = st.columns([1, 1])

            with col1:
                st.write("**âŒ NGãƒ¯ãƒ¼ãƒ‰:**")
                st.code(result['ng_word'], language="text")

                st.write("**âœ… ç½®æ›å€™è£œ:**")
                st.code(result['replacement'], language="text")

            with col2:
                st.write("**ğŸ“‹ è©³ç´°æƒ…å ±:**")
                st.write(f"â€¢ **ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«:** {result['risk_level'].upper()}")
                st.write(f"â€¢ **é¡ä¼¼åº¦:** {result['similarity']:.4f}")
                st.write(f"â€¢ **è·é›¢:** {result['distance']:.4f}")

            st.write("**ğŸ” NGç†ç”±:**")
            st.info(result['reason'])

def batch_check_mode():
    """ãƒãƒƒãƒãƒã‚§ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰"""
    st.subheader("ğŸ“ ãƒãƒƒãƒãƒã‚§ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰")
    st.write("è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€åº¦ã«ãƒã‚§ãƒƒã‚¯ã§ãã¾ã™")

    # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã§è¤‡æ•°è¡Œå…¥åŠ›
    batch_text = st.text_area(
        "ãƒã‚§ãƒƒã‚¯ã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ1è¡Œ1é …ç›®ï¼‰",
        height=200,
        placeholder="ç¾ç™½åŠ¹æœæŠœç¾¤\nãƒ‹ã‚­ãƒ“ãŒæ²»ã‚‹\nã‚¢ãƒ³ãƒã‚¨ã‚¤ã‚¸ãƒ³ã‚°åŠ¹æœ\nè‚Œè’ã‚Œæ”¹å–„\nå‰¯ä½œç”¨ãªã—"
    )

    col1, col2, col3 = st.columns([1, 1, 4])

    with col1:
        batch_threshold = st.slider(
            "é¡ä¼¼åº¦ã—ãã„å€¤",
            min_value=0.1,
            max_value=1.0,
            value=0.3,
            step=0.05
        )

    with col2:
        batch_max_results = st.number_input(
            "æœ€å¤§çµæœæ•°",
            min_value=1,
            max_value=20,
            value=5
        )

    if st.button("ğŸš€ ãƒãƒƒãƒãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ") and batch_text.strip():
        vectorizer, collection = load_vectorizer_and_db()
        if vectorizer is None or collection is None:
            return

        lines = [line.strip() for line in batch_text.split('\n') if line.strip()]

        results_data = []
        progress_bar = st.progress(0)

        for i, line in enumerate(lines):
            results = search_ng_words(line, vectorizer, collection, batch_threshold, batch_max_results)

            if results:
                for result in results:
                    results_data.append({
                        'ãƒã‚§ãƒƒã‚¯å¯¾è±¡': line,
                        'NGãƒ¯ãƒ¼ãƒ‰': result['ng_word'],
                        'ç½®æ›å€™è£œ': result['replacement'],
                        'ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«': result['risk_level'],
                        'é¡ä¼¼åº¦': f"{result['similarity']:.3f}"
                    })
            else:
                # NGãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã‚‚è¨˜éŒ²
                results_data.append({
                    'ãƒã‚§ãƒƒã‚¯å¯¾è±¡': line,
                    'NGãƒ¯ãƒ¼ãƒ‰': '(æ¤œå‡ºãªã—)',
                    'ç½®æ›å€™è£œ': 'å•é¡Œãªã—',
                    'ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«': 'safe',
                    'é¡ä¼¼åº¦': '0.000'
                })

            progress_bar.progress((i + 1) / len(lines))

        if results_data:
            st.write(f"### ğŸ“Š ãƒãƒƒãƒãƒã‚§ãƒƒã‚¯çµæœ: {len(lines)} é …ç›®ã‚’æ¤œæŸ»")

            df = pd.DataFrame(results_data)

            # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«åˆ¥ã®ã‚«ã‚¦ãƒ³ãƒˆ
            risk_counts = df[df['ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«'] != 'safe']['ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«'].value_counts()
            if not risk_counts.empty:
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ğŸ”´ é«˜ãƒªã‚¹ã‚¯", risk_counts.get('high', 0))
                with col2:
                    st.metric("ğŸŸ¡ ä¸­ãƒªã‚¹ã‚¯", risk_counts.get('mid', 0))
                with col3:
                    st.metric("ğŸŸ¢ ä½ãƒªã‚¹ã‚¯", risk_counts.get('low', 0))
                with col4:
                    st.metric("âœ… å®‰å…¨", len(df[df['ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«'] == 'safe']))

            # çµæœã®è¡¨ç¤º
            st.dataframe(df, use_container_width=True)

            # CSVå‡ºåŠ›æ©Ÿèƒ½
            csv = df.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“¥ çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv,
                file_name=f"ngword_check_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    st.title("ğŸš« NGãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  (TF-IDFç‰ˆ)")
    st.write("åºƒå‘Šãƒ»ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æ–‡è¨€ã®NGãƒ¯ãƒ¼ãƒ‰ã‚’TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ã§æ¤œå‡ºã—ã¾ã™")

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ³ã®ç¢ºèª
    if not os.path.exists("./chroma_db"):
        st.error("âŒ ChromaDBãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã« `setup_ngword_db_simple.py` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.code("python setup_ngword_db_simple.py")
        return

    if not os.path.exists("tfidf_vectorizer.pkl"):
        st.error("âŒ TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã« `setup_ngword_db_simple.py` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return

    # è¨­å®šæƒ…å ±ã®è¡¨ç¤º
    if os.path.exists("ngword_db_config.json"):
        with open("ngword_db_config.json", "r", encoding="utf-8") as f:
            config = json.load(f)

        # with st.expander("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±", expanded=False):
        #     col1, col2, col3, col4 = st.columns(4)
        #     with col1:
        #         st.metric("ç·NGãƒ¯ãƒ¼ãƒ‰æ•°", config.get("total_records", "ä¸æ˜"))
        #     with col2:
        #         st.metric("ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ", config.get("vector_dim", "ä¸æ˜"))
        #     with col3:
        #         st.metric("ãƒ¢ãƒ‡ãƒ«", "TF-IDF")
        #     with col4:
        #         st.metric("æœ€çµ‚æ›´æ–°", config.get("created_at", "ä¸æ˜")[:10])

    # ãƒ¡ã‚¤ãƒ³ã‚¿ãƒ–
    tab1, tab2, tab3 = st.tabs(["ğŸ” å˜èªæ¤œç´¢", "ğŸ“ ãƒãƒƒãƒãƒã‚§ãƒƒã‚¯", "ğŸ“ˆ çµ±è¨ˆæƒ…å ±"])

    with tab1:
        st.subheader("ğŸ” å˜èªæ¤œç´¢ãƒ¢ãƒ¼ãƒ‰")

        # æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        col1, col2 = st.columns([3, 1])

        with col1:
            query = st.text_input(
                "æ¤œç´¢ã—ãŸã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
                placeholder="ä¾‹: ç¾ç™½åŠ¹æœã€ãƒ‹ã‚­ãƒ“ãŒæ²»ã‚‹ã€ã‚¢ãƒ³ãƒã‚¨ã‚¤ã‚¸ãƒ³ã‚°ã€è‚Œè’ã‚Œæ”¹å–„"
            )

        with col2:
            search_button = st.button("ğŸ” æ¤œç´¢å®Ÿè¡Œ", type="primary")

        # è©³ç´°è¨­å®š
        with st.expander("âš™ï¸ æ¤œç´¢è¨­å®š", expanded=False):
            col1, col2 = st.columns(2)

            with col1:
                threshold = st.slider(
                    "é¡ä¼¼åº¦ã—ãã„å€¤",
                    min_value=0.1,
                    max_value=1.0,
                    value=0.3,
                    step=0.05,
                    help="ã“ã®å€¤ä»¥ä¸Šã®é¡ä¼¼åº¦ã‚’æŒã¤NGãƒ¯ãƒ¼ãƒ‰ã‚’è¡¨ç¤ºã—ã¾ã™ï¼ˆTF-IDFã§ã¯ä½ã‚ã®å€¤ã‚’æ¨å¥¨ï¼‰"
                )

            with col2:
                max_results = st.number_input(
                    "æœ€å¤§è¡¨ç¤ºä»¶æ•°",
                    min_value=1,
                    max_value=50,
                    value=10
                )

        # æ¤œç´¢å®Ÿè¡Œ
        if (search_button or query) and query.strip():
            vectorizer, collection = load_vectorizer_and_db()

            if vectorizer is not None and collection is not None:
                with st.spinner("NGãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢ä¸­..."):
                    results = search_ng_words(query, vectorizer, collection, threshold, max_results)

                display_search_results(results, query)

                # æ¤œç´¢ã®ãƒ’ãƒ³ãƒˆ
                if not results:
                    st.info("""
                    ğŸ’¡ **æ¤œç´¢ã®ãƒ’ãƒ³ãƒˆ:**
                    - TF-IDFã¯æ–‡å­—ãƒ¬ãƒ™ãƒ«ã®é¡ä¼¼æ€§ã‚’è¦‹ã‚‹ãŸã‚ã€å®Œå…¨ä¸€è‡´ã«è¿‘ã„è¡¨ç¾ãŒã‚ˆã‚Šé«˜ã„é¡ä¼¼åº¦ã‚’ç¤ºã—ã¾ã™
                    - é¡ä¼¼åº¦ã—ãã„å€¤ã‚’ä¸‹ã’ã¦ã¿ã¦ãã ã•ã„ (æ¨å¥¨: 0.1-0.4)
                    - åŒã˜æ„å‘³ã§ã‚‚è¡¨ç¾ãŒç•°ãªã‚‹å ´åˆã¯æ¤œå‡ºã•ã‚Œã«ãã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
                    """)

    with tab2:
        batch_check_mode()

    with tab3:
        st.subheader("ğŸ“ˆ çµ±è¨ˆæƒ…å ±")

        vectorizer, collection = load_vectorizer_and_db()
        if vectorizer is not None and collection is not None:
            total_count = collection.count()
            st.metric("ç·NGãƒ¯ãƒ¼ãƒ‰æ•°", total_count)

            # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            sample_results = collection.get(limit=5)
            if sample_results['metadatas']:
                st.write("### ğŸ“ ã‚µãƒ³ãƒ—ãƒ«NGãƒ¯ãƒ¼ãƒ‰")
                sample_df = pd.DataFrame([
                    {
                        'NGãƒ¯ãƒ¼ãƒ‰': meta['ng_word'],
                        'ç½®æ›å€™è£œ': meta['replacement'][:50] + "..." if len(meta['replacement']) > 50 else meta['replacement'],
                        'ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«': meta['risk_level']
                    }
                    for meta in sample_results['metadatas'][:5]
                ])
                st.dataframe(sample_df, use_container_width=True)

            # # TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã®æƒ…å ±
            # if vectorizer is not None:
            #     st.write("### ğŸ”§ TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼æƒ…å ±")
            #     col1, col2, col3 = st.columns(3)
            #     with col1:
            #         st.metric("N-gramç¯„å›²", "2-4æ–‡å­—")
            #     with col2:
            #         st.metric("æœ€å¤§ç‰¹å¾´é‡", vectorizer.vectorizer.max_features)
            #     with col3:
            #         actual_features = len(vectorizer.vectorizer.get_feature_names_out()) if hasattr(vectorizer.vectorizer, 'get_feature_names_out') else "ä¸æ˜"
            #         st.metric("å®Ÿéš›ã®ç‰¹å¾´é‡", actual_features)

if __name__ == "__main__":
    main()